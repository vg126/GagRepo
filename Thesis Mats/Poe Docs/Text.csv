model_handle,provider,point_cost,context_window_tokens,flags,has_multimodal_input,web_search_type,code_capability_profile,reasoning_profile,comments,,,,,,,,,
Aya-Expanse-32,Cohere,Not accessible,Not Found,None,No,None,Standard,Standard,32B open-weight research release with advanced multilingual capabilities in 23 languages.,,,,,,,,,
Aya-Vision,Cohere,Not Found,Not Found,None,Yes (Image),None,Standard,Standard,"32B open-weights model, excels in 23 languages in vision and text.",,,,,,,,,
Bagoodex-Web-Search,Bagoodex,Not Found,Not Found,None,Yes (Image),Dedicated,Standard,Standard,"Offers instant access to videos, images, weather, and more.",,,,,,,,,
ChatGPT-4o-Latest,OpenAI,345+ points,128000,None,No,None,High,Standard,Dynamic model continuously updated. Cannot generate images.,,,,,,,,,
Claude-Haiku-3,Anthropic,20+ points,Not Found,None,No,None,Standard,Standard,"Outperforms models in its intelligence category on performance, speed and cost.Assistant",Poe,Variable points,Variable,None,Yes (Image),Integrated,Standard,Standard,Routes queries to other models based on task.
Claude-Haiku-3.5,Anthropic,Not available,Not Found,None,No,None,Standard,Standard,Latest generation of Anthropic's fastest model.,,,,,,,,,
Claude-Haiku-3.5-Search,Anthropic,Not available,Not Found,None,No,Integrated,Standard,Standard,Access to real-time information.,,,,,,,,,
Claude-Opus-3,Anthropic,Not accessible,200000,None,No,None,High,Optimized,Handles complex analysis and longer tasks with multiple steps.,,,,,,,,,
Claude-Opus-4,Anthropic,4951+ points,200000,--thinking_budget,No,None,High,Tunable,Customizable thinking budget up to 30k tokens.,,,,,,,,,
Claude-Opus-4-Reasoning,Anthropic,Not available,200000,--thinking_budget,No,None,High,Tunable,Customizable thinking budget up to 30k tokens.,,,,,,,,,
Claude-Opus-4-Search,Anthropic,1134+ points,200000,--thinking_budget,No,Integrated,High,Tunable,Access to real-time information. Thinking budget up to 126k tokens.,,,,,,,,,
Claude-Opus-4.1,Anthropic,Not Found,200000,--thinking_budget,No,None,High,Tunable,Customizable thinking budget up to 32k tokens.,,,,,,,,,
Claude-Sonnet-3.5,Anthropic,297+ points,200000,None,Yes (Image),None,High,Optimized,"October 22, 2024 model snapshot. Excels in visual processing.",,,,,,,,,
Claude-Sonnet-3.5-June,Anthropic,Not accessible,Not Found,None,Yes (Image),None,High,Optimized,Legacy June 2024 snapshot; generally more verbose.,,,,,,,,,
Claude-Sonnet-3.5-Search,Anthropic,Not available,Not Found,None,Yes (Image),Integrated,High,Optimized,Access to real-time information.,,,,,,,,,
Claude-Sonnet-3.7,Anthropic,1044+ points,200000,--thinking_budget,No,None,Standard,Tunable,"Hybrid reasoning model. Thinking budget up to 16,384 tokens.",,,,,,,,,
Claude-Sonnet-3.7-Reasoning,Anthropic,2371+ points,200000,--thinking_budget,No,None,High,Tunable,Reasoning on by default. Recommended for complex math or coding.,,,,,,,,,
Claude-Sonnet-3.7-Search,Anthropic,Not available,200000,--thinking_budget,No,Integrated,Standard,Tunable,Access to real-time information. Thinking budget up to 126k tokens.,,,,,,,,,
Claude-Sonnet-4,Anthropic,938+ points,200000,--thinking_budget,No,None,Standard,Tunable,Customizable thinking budget up to 30k tokens.,,,,,,,,,
Claude-Sonnet-4-Reasoning,Anthropic,1628+ points,200000,--thinking_budget,No,None,Standard,Tunable,Customizable thinking budget up to 60k tokens.,,,,,,,,,
Claude-Sonnet-4-Search,Anthropic,227+ points,200000,--thinking_budget,No,Integrated,Standard,Tunable,Access to real-time information. Thinking budget up to 126k tokens.,,,,,,,,,
Command-R,Cohere,170 points,Not Found,None,No,Integrated,Standard,Standard,Can search the web and respond in over 10 languages.,,,,,,,,,
Command-R-Plus,Cohere,1130 points,Not Found,None,No,Integrated,High,Standard,Supercharged version of Command R.,,,,,,,,,
DeepClaude,DeepClaude,Not Found,Not Found,None,No,None,High,Optimized,Combines DeepSeek R1's CoT with Claude's creative/code generation.,,,,,,,,,
DeepSeek-Prover-V2,,,,,,,,,DeepSeek-Prover-V2 is an open-source large language model specifically designed for formal theorem proving in Lean 4. The model builds on a recursive theorem proving pipeline powered by the company's DeepSeek-V3 foundation model.,,,,,,,,,
DeepSeek-R1,DeepSeek,600 points,164000,None,No,None,High,Optimized,Top open-source reasoning LLM. Data sent to Together AI.,,,,,,,,,
DeepSeek-R1-DI,DeepSeek,200 points,64000,None,No,None,High,Optimized,Quantization: FP8. Data sent to DeepInfra.,,,,,,,,,
DeepSeek-R1-Distill,DeepSeek,150 points,128000,None,No,None,High,Optimized,Fine-tuned version of Llama 3.3 70B served from GroqCloud.,,,,,,,,,
DeepSeek-R1-FW,DeepSeek,600 points,164000,None,No,None,High,Optimized,Explains its chain of thought. Data sent to Fireworks AI.,,,,,,,,,
DeepSeek-R1-N,DeepSeek,Not Found,Not Found,None,No,None,High,Optimized,Does not accept attachments.,,,,,,,,,
DeepSeek-R1-Turbo-DI,DeepSeek,Not accessible,32000,None,No,None,High,Optimized,Turbo model is quantized for higher speeds. Quantization: FP4. Data sent to DeepInfra.,,,,,,,,,
DeepSeek-V3,DeepSeek,415 points,131000,None,No,None,High,Optimized,Top open-source LLM. Data sent to Together AI.,,,,,,,,,
DeepSeek-V3-DI,DeepSeek,Not accessible,64000,None,No,None,High,Optimized,Quantization: FP8. Data sent to DeepInfra.,,,,,,,,,
DeepSeek-V3-Turbo-DI,DeepSeek,Not accessible,32000,None,No,None,High,Optimized,Turbo variant is quantized for higher speeds. Quantization: FP4. Data sent to DeepInfra.,,,,,,,,,
DeepSeek-V3.1,,,,,,,,,"DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. It supports 128k token context window. The Bot does not currently support attachments",,,,,,,,,
DeepSeek-V3.1-N,,,,,,,,,"DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects: - Hybrid thinking mode: One model supports both thinking mode and non-thinking mode by changing the chat template. - Smarter tool calling: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved. - Higher thinking efficiency: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly. - The Bot does not currently support attachments",,,,,,,,,
Gemini-1.5-Flash,Google,Not Found,Not Found,None,"Yes (Image, Video)",None,Standard,Standard,Optimized for speed. One video per message restriction.,,,,,,,,,
Gemini-1.5-Flash-Search,Google,4+ points,Not Found,None,No,Integrated,Standard,Standard,Grounding model currently supports text only.,,,,,,,,,
Gemini-1.5-Pro,Google,32+ points,Not Found,None,"Yes (Image, Video)",None,High,Optimized,"Accepts text, image, and video input. One video per message.",,,,,,,,,
Gemini-1.5-Pro-Search,Google,47+ points,Not Found,None,No,Integrated,High,Optimized,Grounding model currently supports text only.,,,,,,,,,
Gemini-2.0-Flash,Google,7+ points,1000000,None,Yes (Image),Integrated,High,Optimized,Outperforms 1.5 Pro on key benchmarks at twice the speed.,,,,,,,,,
Gemini-2.0-Flash-Lite,Google,5+ points,1000000,None,No,None,Standard,Standard,Spiritual successor to Gemini 1.5 Flash. Does not support web search.,,,,,,,,,
Gemini-2.5-Flash,Google,9+ points,1000000,--thinking_budget,Yes (Image),Integrated,Standard,Tunable,Upgrade in reasoning and search from 2.0 Flash.,,,,,,,,,
Gemini-2.5-Flash-Lite,,,,,,,,,"A lightweight Gemini 2.5 Flash reasoning model optimized for cost efficiency and low latency. Supports web search. Supports 1 million tokens of input context. For more complex queries, use https://poe.com/Gemini-2.5-Pro or https://poe.com/Gemini-2.5-Flash To instruct the bot to use more thinking effort, add --thinking_budget and a number ranging from 0 to 24,576 to the end of your message.",,,,,,,,,
Gemini-2.5-Flash-Lite-Preview,Google,Not Found,1000000,--thinking_budget,Yes (Image),Integrated,Standard,Tunable,Lightweight and cost-efficient version.,,,,,,,,,
Gemini-2.5-Pro,Google,335+ points,1000000,--thinking_budget,Yes (Image),Integrated,High,Tunable,Supports web search.,,,,,,,,,
Gemma-2-27b-T,Google,90 points,Not Found,None,No,None,High,Standard,"For most use cases, Gemini models will produce better results.",,,,,,,,,
Gemma-3-27B,Google,Not accessible,128000,None,Yes (Image),None,High,Optimized,"Successor to Gemma 2, understands over 140 languages.",,,,,,,,,
GLM-4.5,Zhipu AI,Not Found,Not Found,None,Yes (Image),None,High,Optimized,"355B total parameters. Unifies reasoning, coding, and agent capabilities. The GLM-4.5 series models are foundation models designed for intelligent agents. GLM-4.5 has 355 billion total parameters with 32 billion active parameters, while GLM-4.5-Air adopts a more compact design with 106 billion total parameters and 12 billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications. This bot currently does not accept attachments.",,,,,,,,,
GLM-4.5-Air,,,,,,,,,"GLM-4.5-Air is a 106-billion parameter (12B active) foundation model designed for intelligent agent applications, featuring hybrid reasoning capabilities with both thinking and non-thinking modes. It unifies reasoning, coding, and agent functionality while maintaining superior efficiency, achieving competitive performance at 59.8 on industry benchmarks.",,,,,,,,,
GLM-4.5-Air-T,,,,,,,,,"The GLM-4.5 series models are foundation models designed for intelligent agents. GLM-4.5 has 355 billion total parameters with 32 billion active parameters, while GLM-4.5-Air adopts a more compact design with 106 billion total parameters and 12 billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.",,,,,,,,,
GLM-4.5-FW,,,,,,,,,"The GLM-4.5 series models are foundation models designed for intelligent agents. GLM-4.5 has 355 billion total parameters with 32 billion active parameters. It unifies reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.",,,,,,,,,
GPT-3.5-Turbo,OpenAI,14+ points,16384,None,No,None,Standard,Standard,Powerful language generation system.,,,,,,,,,
GPT-3.5-Turbo-Instruct,OpenAI,19+ points,Not Found,None,No,None,Standard,Standard,Powered by gpt-3.5-turbo-instruct.,,,,,,,,,
GPT-3.5-Turbo-Raw,OpenAI,15+ points,Not Found,None,No,None,Standard,Standard,Powered by gpt-3.5-turbo without a system prompt.,,,,,,,,,
GPT-4-Classic,OpenAI,757+ points,Not Found,None,Yes (Image),None,High,Standard,Powered by gpt-4-0613 for text and gpt-4o for images.,,,,,,,,,
GPT-4-Classic-0314,OpenAI,Not accessible,Not Found,None,Yes (Image),None,High,Standard,Powered by gpt-4-0314 for text and gpt-4o for images.,,,,,,,,,
GPT-4-Turbo,OpenAI,406+ points,128000,None,Yes (Image),None,High,Standard,Powered by GPT-4 Turbo with Vision.,,,,,,,,,
GPT-4.1,OpenAI,196+ points,1000000,None,Yes (Image),None,High,Standard,75% chat history cache discount.,,,,,,,,,
GPT-4.1-mini,OpenAI,28+ points,1000000,None,Yes (Image),None,Standard,Standard,"Small, fast & affordable model that matches or beats GPT-4o.",,,,,,,,,
GPT-4.1-nano,OpenAI,8+ points,1000000,None,Yes (Image),None,Standard,Standard,Extremely fast and cheap model for summarization/categorization.,,,,,,,,,
GPT-4o,OpenAI,Variable points,128000,None,Yes (Image),None,High,Standard,Uses GPT-Image-1 for image generation.,,,,,,,,,
GPT-4o-Aug,OpenAI,300 points,Not Found,None,Yes (Image),None,High,Standard,August 2024 model snapshot of GPT-4o.,,,,,,,,,
GPT-4o-mini,OpenAI,7+ points,Not Found,None,No,None,Standard,Standard,"Significantly smarter, cheaper, and as fast as GPT-3.5 Turbo.",,,,,,,,,
GPT-4o-mini-Search,OpenAI,836+ points,128000,None,No,Integrated,Standard,Standard,Less expensive version of GPT-4o-Search. Does not support image search.,,,,,,,,,
GPT-4o-Search,OpenAI,1235+ points,128000,None,No,Integrated,High,Standard,Fine-tuned for searching the web. Does not support image search.,,,,,,,,,
GPT-5,OpenAI,Not Found,400000,--reasoning_effort,Yes (Image),None,High,Tunable,90% chat history cache discount.,,,,,,,,,
GPT-5-Chat,OpenAI,Not Found,Not Found,None,Yes (Image),None,High,Standard,Snapshot used in ChatGPT; 90% chat history cache discount.,,,,,,,,,
GPT-5-mini,OpenAI,Not Found,400000,--reasoning_effort,Yes (Image),None,Standard,Tunable,Matches or beats GPT-4.1 in many tasks. 90% chat history cache discount.,,,,,,,,,
GPT-5-nano,OpenAI,Not Found,400000,--reasoning_effort,Yes (Image),None,Standard,Tunable,Extremely fast model for summarization/categorization. 90% chat history cache discount.,,,,,,,,,
GPT-OSS-120B,OpenAI,Not Found,Not Found,None,No,Integrated,Specialized,Optimized,Apache 2.0 license. Text-only. Does not support attachments.,,,,,,,,,
GPT-OSS-120B-CS,OpenAI,Not Found,Not Found,None,Yes (Image),Integrated,Specialized,Optimized,WorldÃ¢ÂÂs fastest inference for GPT OSS 120B with Cerebras.,,,,,,,,,
GPT-OSS-120B-T,OpenAI,Not Found,Not Found,None,No,None,Specialized,Optimized,Apache 2.0 license. Fully open model.,,,,,,,,,
GPT-OSS-20B,OpenAI,Not Found,Not Found,None,No,Integrated,Specialized,Optimized,Apache 2.0 license. Designed for edge devices. Does not accept attachments.,,,,,,,,,
GPT-OSS-20B-T,OpenAI,Not Found,Not Found,None,No,None,Specialized,Optimized,Apache 2.0 license. Designed for single-GPU deployment.,,,,,,,,,
GPT-Researcher,Tavily,Variable points,Not Found,None,No,Dedicated,Standard,Standard,Powered by Tavily's search engine. Based on open source project.,,,,,,,,,
Grok-2,xAI,209+ points,Not Found,None,Yes (Image),None,High,Optimized,Does not have access to real-time information from X or the internet.,,,,,,,,,
Grok-3,xAI,907+ points,131000,None,Yes (Image),None,High,Optimized,Uses Grok 2 for native vision. No access to X data feed on Poe.,,,,,,,,,
Grok-3-Mini,xAI,39+ points,131000,--reasoning_effort,Yes (Image),None,Standard,Tunable,No access to X data feed on Poe.,,,,,,,,,
Grok-4,xAI,Not Found,Not Found,None,Yes (Image),None,High,Optimized,State-of-the-art in coding and reasoning.,,,,,,,,,
Hermes-3-70B,Nous Research,Not accessible,Not Found,None,No,None,High,Optimized,"Advanced agentic capabilities, much better roleplaying, and reasoning.",,,,,,,,,
Inception-Mercury-Coder,Inception Labs,14+ points,Not Found,None,No,None,Specialized,Optimized,First diffusion large language model (dLLM). Runs 5-10x faster than comparable models.,,,,,,,,,
Kimi-K2,Moonshot AI,Not Found,Not Found,None,No,None,High,Optimized,"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.",,,,,,,,,
Kimi-K2-Instruct,,,,,,,,,"Kimi K2 Instruct is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. Key Features: - Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability. - MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up. - Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.",,,,,,,,,
Kimi-K2-T,Moonshot AI,Not Found,Not Found,None,No,None,High,Optimized,Mixture-of-experts (MoE) model.,,,,,,,,,
Llama-3-70B-FP16,Meta,Not accessible,128000,None,No,None,High,Standard,A highly efficient and powerful model designed for a variety of tasks.,,,,,,,,,
Llama-3-70b-Groq,Meta,75 points,Not Found,None,No,None,High,Standard,Powered by the Groq LPUÃ¢ÂÂ¢ Inference Engine.,,,,,,,,,
Llama-3-70B-T,Meta,75 points,Not Found,None,No,None,High,Standard,"For most use cases, Llama-3.3-70B will perform better.",,,,,,,,,
Llama-3-8b-Groq,Meta,10 points,Not Found,None,No,None,Standard,Standard,Llama 3 8b powered by the Groq LPUÃ¢ÂÂ¢ Inference Engine.,,,,,,,,,
Llama-3-8B-T,Meta,15 points,Not Found,None,No,None,Standard,Standard,The points price is subject to change.,,,,,,,,,
Llama-3.1-405B,Meta,65+ points,Not Found,None,No,None,High,Standard,Pinnacle of Meta's Llama 3.1 family. Optimized for chat use cases.,,,,,,,,,
Llama-3.1-405B-FP16,Meta,Not accessible,128000,None,No,None,High,Standard,The biggest and best open-source AI model trained by Meta; BF16 precision.,,,,,,,,,
Llama-3.1-405B-FW,Meta,Not accessible,128000,None,No,None,High,Standard,Instruction tuned text only models.,,,,,,,,,
Llama-3.1-405B-T,Meta,335 points,128000,None,No,None,High,Standard,Points price is subject to change.,,,,,,,,,
Llama-3.1-70B,Meta,18+ points,Not Found,None,No,None,High,Standard,Context window shortened to optimize for speed and cost.,,,,,,,,,
Llama-3.1-70B-FP16,Meta,Not accessible,128000,None,No,None,High,Standard,Faster response times compared to the 405B model.,,,,,,,,,
Llama-3.1-70B-FW,Meta,Not accessible,128000,None,No,None,High,Standard,Optimized for multilingual dialogue use cases.,,,,,,,,,
Llama-3.1-70B-T,Meta,Not accessible,128000,None,No,None,High,Standard,Points price is subject to change.,,,,,,,,,
Llama-3.1-8B,Meta,20 points,Not Found,None,No,None,Standard,Standard,Context window has been shortened to optimize for speed and cost.,,,,,,,,,
Llama-3.1-8B-DI,Meta,Not accessible,128000,None,No,None,Standard,Standard,Quantization: FP16. Data sent to DeepInfra.,,,,,,,,,
Llama-3.1-8B-FP16,Meta,50 points,128000,None,No,None,Standard,Standard,Smallest and fastest member of the Llama 3.1 family; 128K context length.,,,,,,,,,
Llama-3.1-8B-FW,Meta,Not accessible,128000,None,No,None,Standard,Standard,Optimized for multilingual dialogue use cases.,,,,,,,,,
Llama-3.1-8B-T-128k,Meta,100 points,128000,None,No,None,Standard,Standard,Points price is subject to change.,,,,,,,,,
Llama-3.1-Nemotron,NVIDIA,200 points,Not Found,None,No,None,High,Optimized,"Excels in understanding, following instructions, writing and performing coding tasks.",,,,,,,,,
Llama-3.3-70B,Meta,130 points,Not Found,None,No,None,High,Standard,Similar performance as Llama 3.1 405B while being faster and smaller.,,,,,,,,,
Llama-3.3-70B-DI,Meta,Not accessible,128000,None,No,None,High,Standard,Quantization: FP8. Data sent to DeepInfra.,,,,,,,,,
Llama-3.3-70B-FW,Meta,140 points,Not Found,None,No,None,High,Standard,Hosted by Fireworks AI. Delivers leading performance for text-based use cases.,,,,,,,,,
Llama-3.3-70B-Vers,Meta,Not Found,Not Found,None,Yes (Image),None,High,Standard,"Supports analyzing images, PDFs, SVGs, XLSX, WEBP, HTML, etc.",,,,,,,,,
Llama-4-Maverick,Meta,50 points,1050000,None,Yes (Image),None,Standard,Standard,SOTA intelligence and fast performance.,,,,,,,,,
Llama-4-Maverick-B10,Meta,Not accessible,1000000,--page_range,Yes (Image),None,High,Standard,Ultra-fast implementation by Baseten. Supports images and PDFs.,,,,,,,,,
Llama-4-Maverick-T,Meta,Not accessible,500000,None,Yes (Image),None,Standard,Standard,128-expert MoE powerhouse for multilingual image/text understanding.,,,,,,,,,
Llama-4-Scout,Meta,30 points,131000,None,Yes (Image),None,High,Standard,"Versatile, general-purpose LLM.",,,,,,,,,
Llama-4-Scout-B10,Meta,100 points,8000000,--page_range,Yes (Image),None,High,Standard,Largest context window on Poe. Supports images and PDFs.,,,,,,,,,
Llama-4-Scout-CS,Meta,Not Found,Not Found,None,Yes (Image),None,High,Standard,WorldÃ¢ÂÂs fastest inference for Llama 4 Scout with Cerebras.,,,,,,,,,
Llama-4-Scout-T,Meta,35 points,300000,None,Yes (Image),None,High,Standard,"16-expert MoE model, excels at multi-document analysis.",,,,,,,,,
Magistral-Medium-2506-Thinking,,,,,,,,,"Magistral Medium 2506 (thinking) by Empiriolabs. Magistral is Mistral's first reasoning model. It is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling — this model solves multi-step challenges where transparency and precision are critical. Context Window: 40,000k Supported file type uploads: PDF, XLSX, TXT, PNG, JPG, JPEG",,,,,,,,,
MiniMax-M1,MiniMax,100+ points,1000000,None,No,None,Standard,Optimized,Pure text reasoning model; does not process any file types.,,,,,,,,,
Mistral-7B-v0.3-DI,Mistral AI,Not accessible,32000,None,No,None,Standard,Standard,Quantization: FP16. Data sent to DeepInfra.,,,,,,,,,
Mistral-7B-v0.3-T,Mistral AI,45 points,Not Found,None,No,None,Standard,Standard,Points price is subject to change.,,,,,,,,,
Mistral-Large-2,Mistral AI,231+ points,128000,None,No,None,High,Optimized,Top-tier reasoning capabilities for complex multilingual tasks.,,,,,,,,,
Mistral-Medium,Mistral AI,181+ points,32000,None,No,None,Standard,Standard,Stronger than Mixtral-8x7b and Mistral-7b on benchmarks.,,,,,,,,,
Mistral-Medium-3,,,,,,,,,"Mistral Medium 3 is a powerful, cost-efficient language model offering top-tier reasoning and multimodal performance. Context Window: 130k",,,,,,,,,
Mistral-NeMo,Mistral AI / NVIDIA,Not Found,Not Found,None,Yes (Image),None,High,Standard,12B parameter open-source model with extensive multilingual support.,,,,,,,,,
Mistral-Small-3,Mistral AI,10+ points,Not Found,None,No,None,Standard,Standard,Apache 2.0 license. Comparable to Llama-3.3-70B.,,,,,,,,,
Mistral-Small-3.1,Mistral AI,Not accessible,128000,None,Yes (Image),None,High,Optimized,24B parameter model with advanced multimodal capabilities.,,,,,,,,,
Mistral-Small-3.2,Mistral AI,Not Found,Not Found,None,Yes (Image),None,Standard,Standard,"Description is in German; Open-source, runs on modest hardware.",,,,,,,,,
Mixtral8x22b-Inst-FW,Mistral AI,120 points,Not Found,None,No,None,High,Standard,Mixture-of-Experts instruct model from Mistral hosted by Fireworks.,,,,,,,,,
model_handle,provider,point_cost,context_window_tokens,flags,has_multimodal_input,web_search_type,code_capability_profile,reasoning_profile,comments,,,,,,,,,
o1,OpenAI,4120+ points,200000,--reasoning_effort,Yes (Image),None,High,Tunable,Designed to reason before it responds.,,,,,,,,,
o1-mini,OpenAI,385+ points,128000,None,Yes (Image),None,High,Optimized,Small version of OpenAI's o1 model.,,,,,,,,,
o1-pro,OpenAI,54420+ points,Not Found,--reasoning_effort,Yes (Image),None,High,Tunable,"Tailored for complex, compute- or context-heavy tasks.",,,,,,,,,
o3,OpenAI,425+ points,200000,--reasoning_effort,Yes (Image),None,High,Tunable,State-of-the-art intelligence.,,,,,,,,,
o3-deep-research,OpenAI,95410+ points,Not Found,None,No,Dedicated,Standard,Standard,Powered by o3 model. For complex research questions.,,,,,,,,,
o3-mini,OpenAI,244+ points,200000,--reasoning_effort,Yes (Image),None,High,Tunable,Uses medium reasoning effort by default.Tako,,,,,,,,,
o3-mini-high,OpenAI,508+ points,200000,--reasoning_effort,Yes (Image),None,High,Tunable,reasoning_effort is set to high by default.,,,,,,,,,
o3-pro,OpenAI,4242+ points,Not Found,--reasoning_effort,Yes (Image),None,High,Tunable,"Especially capable at math, science, and coding.",,,,,,,,,
o4-mini,OpenAI,255+ points,200000,--reasoning_effort,Yes (Image),None,High,Tunable,"High intelligence for science, math, and coding.",,,,,,,,,
o4-mini-deep-research,OpenAI,109+ points,Not Found,None,No,Dedicated,Standard,Standard,Powered by o4-mini model. For complex research questions.,,,,,,,,,
OpenAI-GPT-OSS-120B,OpenAI,Not Found,Not Found,--reasoning_effort,No,Integrated,Specialized,Tunable,Trained on Harmony response format; fits on a single H100 GPU.,,,,,,,,,
OpenAI-GPT-OSS-20B,OpenAI,Not Found,Not Found,--reasoning_effort,No,Integrated,Specialized,Tunable,Compact model for low-latency and edge deployments.,,,,,,,,,
Perplexity-Deep-Research,Perplexity,15167 points,128000,None,No,Dedicated,Standard,Optimized,Research-focused model for multi-step retrieval and synthesis.,,,,,,,,,
Perplexity-R1-1776,Perplexity,Not Found,128000,None,No,None,High,Optimized,DeepSeek-R1 model post-trained to remove CCP censorship. No web search.,,,,,,,,,
Perplexity-Sonar,Perplexity,Not accessible,127000,None,No,Dedicated,Standard,Standard,"Delivers real-time, web-connected search results with citations.",,,,,,,,,
Perplexity-Sonar-Pro,Perplexity,1667 points,200000,None,No,Dedicated,Standard,Standard,Enhanced version with double the citations and larger context.,,,,,,,,,
Perplexity-Sonar-Rsn,Perplexity,1234 points,128000,None,No,Dedicated,Standard,Optimized,Operates on the open-sourced uncensored R1-1776 model.,,,,,,,,,
Perplexity-Sonar-Rsn-Pro,Perplexity,2967 points,128000,None,No,Dedicated,Standard,Optimized,Operates on the open-sourced uncensored R1-1776 model.,,,,,,,,,
Phi-4-DI,Microsoft,Not accessible,16000,None,No,None,Standard,Optimized,14B parameter model. Works best with English. Data sent to DeepInfra.,,,,,,,,,
Qwen-2.5-72B-T,Alibaba,Not accessible,Not Found,None,No,None,High,Standard,Results on par with Llama-3-405B despite using only one-fifth of the parameters.,,,,,,,,,
Qwen-2.5-7B-T,Alibaba,Not accessible,Not Found,None,No,None,High,Standard,"Excels in coding, math, instruction following, and has great multilingual support.",,,,,,,,,
Qwen-2.5-Coder-32B-T,Alibaba,210 points,Not Found,None,No,None,Specialized,Standard,"A powerful model with 32.5B parameters, excelling in coding and math.",,,,,,,,,
Qwen-2.5-VL-32b,Alibaba,Not accessible,Not Found,None,Yes (Image),None,Standard,Optimized,Strengthened math/problem-solving through reinforcement learning.,,,,,,,,,
Qwen-3-235B-2507-T,Alibaba,Not Found,262000,None,No,None,High,Optimized,Best instruct model (non-reasoning) among many sources.,,,,,,,,,
Qwen-72B-T,Alibaba,125 points,Not Found,None,No,None,High,Standard,Excels particularly in Chinese-language queries. (Qwen1.5),,,,,,,,,
Qwen2-72B-Instruct-T,Alibaba,190 points,Not Found,None,No,None,High,Standard,Excels particularly in Chinese-language queries.,,,,,,,,,
Qwen2.5-Coder-32B,Alibaba,50 points,Not Found,None,No,None,Specialized,Standard,Latest series of code-specific Qwen large language models.,,,,,,,,,
Qwen2.5-VL-72B-T,Alibaba,Not accessible,32000,None,"Yes (Image, Video)",None,High,Optimized,Excels in visual and video understanding.,,,,,,,,,
Qwen3-235B-2507-CS,Alibaba,Not Found,Not Found,None,No,None,High,Standard,World's fastest inference with Cerebras.,,,,,,,,,
Qwen3-235B-2507-FW,Alibaba,Not accessible,256000,None,No,None,High,Standard,Operates in non-thinking mode. Data sent to Fireworks AI.,,,,,,,,,
Qwen3-235B-A22B,Alibaba,0 points,Not Found,None,No,None,High,Optimized,Fastest implementation of the new Qwen3 235B flagship model.,,,,,,,,,
Qwen3-235B-A22B-DI,Alibaba,Not accessible,32000,None,No,None,High,Optimized,Quantization: FP8.,,,,,,,,,
Qwen3-235B-A22B-N,Alibaba,Not Found,262000,None,No,None,High,Standard,"Does not implement ""thinking mode"". Does not support attachments.",,,,,,,,,
Qwen3-235B-Think-CS,,,,,,,,,"World’s fastest inference for Qwen 235B Thinking (2507) model with Cerebras. Qwen3-235B-A22B-Thinking-2507 is a high-performance, open-weight Mixture-of-Experts (MoE) language model optimized for complex reasoning tasks.. This ""thinking-only"" variant enhances structured logical reasoning, mathematics, science, and long-form generation.",,,,,,,,,
Qwen3-30B-A3B-Instruct,,,,,,,,,"Qwen3-30B-A3B-Instruct-2507 is a 30-billion parameter general-purpose LLM with 256K token context length. It delivers enhanced instruction following, logical reasoning, mathematics, and multilingual capabilities, with better alignment for subjective and open-ended tasks. Uses the latest July 2025 snapshot.",,,,,,,,,
Qwen3-32B-CS,Alibaba,Not Found,Not Found,None,No,None,High,Standard,WorldÃ¢ÂÂs fastest inference for Qwen 3 32B with Cerebras.,,,,,,,,,
Qwen3-480B-Coder-CS,,,,,,,,,"World’s fastest inference for Qwen Coder 480B with Cerebras. Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories.",,,,,,,,,
Qwen3-Coder,,,,,,,,,"Qwen3 Coder 480B A35B Instruct is a state-of-the-art 480B-parameter Mixture-of-Experts model (35B active) that achieves top-tier performance across multiple agentic coding benchmarks. Supports 256K native context length and scales to 1M tokens with extrapolation. All data provided will not be used in training, and is sent only to Fireworks AI, a US-based company.",,,,,,,,,
Qwen3-Coder-30B-A3B,,,,,,,,,"Qwen3-Coder-30B-A3B-Instruct is a 30-billion parameter coding-specialized LLM with 256K token context length, enabling repository-scale code understanding. It excels at autonomous coding tasks and agentic workflows, capable of writing, debugging, and executing complex programming operations independently.",,,,,,,,,
Qwen3-Coder-480B-FW,Alibaba,Not Found,256000,None,No,None,Specialized,Optimized,Scales to 1M tokens with extrapolation. Data sent to Fireworks AI.,,,,,,,,,
Qwen3-Coder-480B-N,Alibaba,Not Found,256000,None,No,None,Specialized,Optimized,Claude Sonnet-comparable performance on agentic coding.,,,,,,,,,
Qwen3-Coder-480B-T,Alibaba,Not Found,262144,None,No,None,Specialized,Optimized,480B total parameters MoE model for code generation.,,,,,,,,,
QwQ-32B-B10,Alibaba,Not accessible,131072,None,No,None,High,Optimized,Medium-sized reasoning model from the Qwen series. Blazing-fast speed.,,,,,,,,,
QwQ-32B-Preview-T,Alibaba,320 points,Not Found,None,No,None,High,Optimized,Experimental research model focused on advancing AI reasoning capabilities.,,,,,,,,,
QwQ-32B-T,Alibaba,Not accessible,131000,None,No,None,High,Optimized,"Compact, 32B open-source reasoning model.",,,,,,,,,
Reka-Core,Reka,1250 points,8000,None,"Yes (Image, Video)",None,Standard,Standard,Reka's largest and most capable multimodal model.,,,,,,,,,
Reka-Flash,Reka,40 points,Not Found,None,"Yes (Image, Video)",None,Standard,Standard,Efficient 21B multimodal model optimized for fast workloads.,,,,,,,,,
Reka-Research,Reka,Not Found,Not Found,None,No,Dedicated,Standard,Optimized,State-of-the-art agentic AI that browses the web.,,,,,,,,,
Solar-Pro-2,Upstage,Not Found,64000,None,No,Integrated,High,Optimized,31B parameter model with world-class multilingual support.,,,,,,,,,
Tako,,Not accessible,Not Found,--specificity,No,Dedicated,Standard,Standard,"Transforms questions into interactive, shareable knowledge cards.",,,,,,,,,
Web-Search,Google,Variable points,Not Found,None,No,Dedicated,Standard,Standard,Powered by Gemini 2.0 Flash.Deepseek-V3-FW,DeepSeek,300 points,131000,None,No,None,High,Optimized,"Data sent to Fireworks, a US-based company."